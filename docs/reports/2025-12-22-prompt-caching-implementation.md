# Weekly Development Report: Anthropic Prompt Caching Implementation

**Report Date**: December 22, 2025
**Author**: Development Team
**Project**: SEDAILY.AI Services Optimization
**Period**: December 21-22, 2025

---

## Executive Summary

This report documents the implementation of a **dual-layer caching strategy** across all 6 SEDAILY.AI services, achieving an **81% reduction in monthly API costs**.

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Monthly Cost (6 services, 1K req each) | $1,389 | $262 | **-$1,127 (81%)** |
| Anthropic Cache HIT Rate | 0% | 100%* | - |
| DynamoDB Query Frequency | Every 5 min | Container restart only | **~99% reduction** |

*After initial cache creation (first request is always MISS)

---

## 1. Optimization Overview

### 1.1 Dual-Layer Caching Strategy

Two independent caching layers were implemented:

| Layer | Target | Location | TTL | Purpose |
|-------|--------|----------|-----|---------|
| **Layer 1** | DynamoDB Prompts | Lambda Memory | Container lifetime (~15min+) | Reduce DB queries |
| **Layer 2** | System Prompt Tokens | Anthropic Server | ~5 min (ephemeral) | Reduce API token costs |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        REQUEST FLOW                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  [User Request]                                                  â”‚
â”‚        â”‚                                                         â”‚
â”‚        â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ LAYER 1: In-Memory Cache (Lambda)                        â”‚    â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚ â”‚ Cache HIT?      â”‚â”€â”€â”€â–¶â”‚ Return cached prompt        â”‚  â”‚    â”‚
â”‚  â”‚ â”‚ (permanent)     â”‚ Y  â”‚ (skip DynamoDB)             â”‚  â”‚    â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â”‚          â”‚ N                                             â”‚    â”‚
â”‚  â”‚          â–¼                                               â”‚    â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚ â”‚ Fetch from DynamoDB â†’ Store in memory permanently   â”‚ â”‚    â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚        â”‚                                                         â”‚
â”‚        â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ LAYER 2: Anthropic Prompt Cache                          â”‚    â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚ â”‚ Cache HIT?      â”‚â”€â”€â”€â–¶â”‚ 90% token cost discount     â”‚  â”‚    â”‚
â”‚  â”‚ â”‚ (~5min TTL)     â”‚ Y  â”‚ ($0.50/1M vs $5.00/1M)      â”‚  â”‚    â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â”‚          â”‚ N                                             â”‚    â”‚
â”‚  â”‚          â–¼                                               â”‚    â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚ â”‚ Cache MISS â†’ Create new cache (one-time cost)       â”‚ â”‚    â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚        â”‚                                                         â”‚
â”‚        â–¼                                                         â”‚
â”‚  [AI Response]                                                   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Services Optimized

| Service | URL | Lambda Function | Cached Tokens |
|---------|-----|-----------------|---------------|
| b1 (Buddy) | https://b1.sedaily.ai | p2-two-websocket-message-two | 18,614 |
| p1 (Proofreading) | https://p1.sedaily.ai | nx-wt-prf-websocket-message | 65,153 |
| r1 (Column) | https://r1.sedaily.ai | sedaily-column-websocket-message | 7,232 |
| w1 (Bodo) | https://w1.sedaily.ai | w1-websocket-message | 19,476 |
| t1 (Title) | https://t1.sedaily.ai | nx-tt-dev-ver3-websocket-message | 74,615 |
| f1 (Foreign) | https://f1.sedaily.ai | f1-websocket-message-two | 63,409 |

### 1.3 Technical Stack

- **AI Model**: Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- **API Provider**: Anthropic Direct API
- **Cloud Platform**: AWS (Lambda, API Gateway, DynamoDB)
- **Region**: us-east-1
- **Runtime**: Python 3.11+

---

## 2. Implementation Details: Before vs After

### 2.1 Layer 1: In-Memory Cache (`websocket_service.py`)

#### BEFORE: 5-Minute TTL Cache

```python
# Global cache with timestamp tracking
PROMPT_CACHE: Dict[str, Tuple[Dict[str, Any], float]] = {}
CACHE_TTL = 300  # 5 minutes

def _load_prompt_from_dynamodb(self, engine_type: str) -> Dict[str, Any]:
    global PROMPT_CACHE
    now = time.time()

    # Check cache with TTL expiration
    if engine_type in PROMPT_CACHE:
        cached_data, cached_time = PROMPT_CACHE[engine_type]
        age = now - cached_time

        if age < CACHE_TTL:
            logger.info(f"Cache HIT (age: {age:.1f}s)")
            return cached_data
        else:
            logger.info(f"Cache EXPIRED (age: {age:.1f}s)")

    # Fetch from DB
    prompt_data = self._fetch_prompt_from_db(engine_type)
    PROMPT_CACHE[engine_type] = (prompt_data, now)  # Store with timestamp
    return prompt_data
```

**Problems:**
- DynamoDB query every 5 minutes
- Timestamp management overhead
- Cache expiration even with unchanged prompts

#### AFTER: Permanent Cache (No TTL)

```python
# Global cache - permanent until Lambda container restart
PROMPT_CACHE: Dict[str, Dict[str, Any]] = {}
# CACHE_TTL removed - permanent cache

def _load_prompt_from_dynamodb(self, engine_type: str) -> Dict[str, Any]:
    global PROMPT_CACHE

    # Simple cache check (no TTL)
    if engine_type in PROMPT_CACHE:
        logger.info(f"âœ… Cache HIT for {engine_type} - DB query skipped (permanent cache)")
        return PROMPT_CACHE[engine_type]

    logger.info(f"âŒ Cache MISS for {engine_type} - fetching from DB (first time)")

    # Fetch from DB and store permanently
    prompt_data = self._fetch_prompt_from_db(engine_type)
    PROMPT_CACHE[engine_type] = prompt_data  # No timestamp needed
    return prompt_data
```

**Improvements:**
- DynamoDB query only on cold start
- Simplified code (no timestamp tracking)
- Cache persists for container lifetime

#### Summary: Layer 1 Changes

| Metric | Before | After |
|--------|--------|-------|
| Cache Structure | `Dict[str, Tuple[Dict, float]]` | `Dict[str, Dict]` |
| TTL | 300 seconds | None (permanent) |
| DB Query Frequency | Every 5 minutes | Container restart only |
| Timestamp Tracking | Required | Not needed |

---

### 2.2 Layer 2: Anthropic Prompt Cache (`anthropic_client.py`)

#### BEFORE: No Caching

```python
# Standard API request without caching
headers = {
    "x-api-key": api_key,
    "anthropic-version": "2023-06-01",
    "content-type": "application/json"
}

body = {
    "model": MODEL_ID,
    "max_tokens": MAX_TOKENS,
    "system": system_prompt,  # Plain string, not cached
    "messages": [{"role": "user", "content": user_message}],
    "stream": True
}
```

**Problems:**
- Full token cost ($5.00/1M) for every request
- No reuse of static prompt content
- High cost for large system prompts

#### AFTER: With Prompt Caching

```python
# API request with prompt caching enabled
headers = {
    "x-api-key": api_key,
    "anthropic-version": "2023-06-01",
    "anthropic-beta": "prompt-caching-2024-07-31",  # Required header
    "content-type": "application/json"
}

body = {
    "model": MODEL_ID,
    "max_tokens": MAX_TOKENS,
    "system": [
        {
            "type": "text",
            "text": static_system_prompt,
            "cache_control": {"type": "ephemeral"}  # Enable caching
        }
    ],
    "messages": [{"role": "user", "content": enhanced_user_message}],
    "stream": True
}
```

**Improvements:**
- 90% cost reduction on cached tokens ($0.50/1M vs $5.00/1M)
- Static prompt reused across requests
- Cache managed automatically by Anthropic (~5 min TTL)

#### Summary: Layer 2 Changes

| Metric | Before | After |
|--------|--------|-------|
| Header | Standard | + `anthropic-beta` |
| System Format | Plain string | Array with `cache_control` |
| Token Cost (cached) | $5.00/1M | $0.50/1M |
| Cache TTL | N/A | ~5 minutes (Anthropic managed) |

---

### 2.3 Static/Dynamic Context Separation

#### BEFORE: Mixed Context (Cache Invalidation)

```python
# Dynamic content in system prompt = cache invalidated every request
system_prompt = f"""
{instruction}
{description}
{files_content}

Current Time: {datetime.now()}           # Dynamic - invalidates cache!
Session ID: {uuid.uuid4()}               # Dynamic - invalidates cache!
Conversation History: {conversation_history}  # Dynamic - invalidates cache!
"""
```

**Problem:** Cache key changes every request due to dynamic content.

#### AFTER: Separated Context (Cache Preserved)

```python
# Static content in system prompt (cached)
def _replace_template_variables(prompt: str) -> str:
    """Static values only - safe for caching"""
    replacements = {
        '{{user_location}}': 'ëŒ€í•œë¯¼êµ­',
        '{{timezone}}': 'Asia/Seoul (KST)',
        '{{language}}': 'í•œêµ­ì–´'
    }
    for key, value in replacements.items():
        prompt = prompt.replace(key, value)
    return prompt

# Dynamic content in user message (not cached)
def _create_dynamic_context() -> str:
    """Dynamic values - added to user message to preserve cache"""
    current_time = datetime.now(kst).strftime('%Y-%m-%d %H:%M:%S KST')
    session_id = str(uuid.uuid4())[:8]
    return f"""[í˜„ìž¬ ì„¸ì…˜ ì •ë³´]
- í˜„ìž¬ ì‹œê°„: {current_time}
- ì„¸ì…˜ ID: {session_id}"""

# Conversation history passed as messages array (not in system prompt)
if conversation_context:
    user_message = f"{conversation_context}\n\n[í˜„ìž¬ ì§ˆë¬¸]\n{user_message}"
```

#### Summary: Context Separation

| Content Type | Before | After |
|--------------|--------|-------|
| Static Config | In system prompt | In system prompt (cached) |
| Current Time | In system prompt | In user message |
| Session ID | In system prompt | In user message |
| Conversation History | In system prompt | In messages array |

---

### 2.4 Cost Calculation Function (New)

```python
def _calculate_cost(self, usage: Dict[str, int]) -> float:
    """
    Calculate API cost based on Claude Opus 4.5 pricing

    Pricing (per 1M tokens):
    - Base Input: $5.00
    - Output: $25.00
    - Cache Write: $10.00 (one-time)
    - Cache Read: $0.50 (90% discount)
    """
    PRICE_INPUT = 5.0
    PRICE_OUTPUT = 25.0
    PRICE_CACHE_WRITE = 10.0
    PRICE_CACHE_READ = 0.50

    cost_input = (usage.get('input_tokens', 0) / 1_000_000) * PRICE_INPUT
    cost_output = (usage.get('output_tokens', 0) / 1_000_000) * PRICE_OUTPUT
    cost_cache_write = (usage.get('cache_creation_input_tokens', 0) / 1_000_000) * PRICE_CACHE_WRITE
    cost_cache_read = (usage.get('cache_read_input_tokens', 0) / 1_000_000) * PRICE_CACHE_READ

    return cost_input + cost_output + cost_cache_write + cost_cache_read
```

---

### 2.5 Cache Usage Logging (New)

```python
# Track and log cache performance
if usage_info['cache_read_input_tokens'] > 0:
    logger.info(f"ðŸŽ¯ PROMPT CACHE HIT! Read {usage_info['cache_read_input_tokens']} tokens from cache")

if usage_info['cache_creation_input_tokens'] > 0:
    logger.info(f"ðŸ’¾ PROMPT CACHE MISS! Created cache with {usage_info['cache_creation_input_tokens']} tokens")

# Log cost breakdown
logger.info(f"ðŸ’° Cost Breakdown:")
logger.info(f"  - Input: ${cost_input:.6f} ({usage.get('input_tokens', 0)} tokens)")
logger.info(f"  - Output: ${cost_output:.6f} ({usage.get('output_tokens', 0)} tokens)")
logger.info(f"  - Cache Write: ${cost_cache_write:.6f}")
logger.info(f"  - Cache Read: ${cost_cache_read:.6f}")
logger.info(f"  - TOTAL: ${total_cost:.6f}")
```

---

## 3. Bug Fixes Applied

| Issue | Root Cause | Before | After |
|-------|------------|--------|-------|
| Invalid cache_control | `ttl` parameter not supported | `{"type": "ephemeral", "ttl": "1h"}` | `{"type": "ephemeral"}` |
| Cache invalidation per turn | Conversation history in system prompt | History in system_prompt | History in messages array |
| Logger undefined | Missing import | `logging.getLogger()` | `setup_logger()` |
| f1 response cutoff | Lambda timeout insufficient | 120 seconds | 180 seconds |

---

## 4. Cost Analysis

### 4.1 Pricing Model (Claude Opus 4.5)

| Token Type | Price (per 1M tokens) | Notes |
|------------|----------------------|-------|
| Input Tokens | $5.00 | Standard rate |
| Output Tokens | $25.00 | Standard rate |
| Cache Read | $0.50 | **90% discount** |
| Cache Write | $10.00 | One-time cost per cache creation |

### 4.2 Per-Request Cost Comparison

| Service | Cached Tokens | Before | After | Savings |
|---------|---------------|--------|-------|---------|
| b1 (Buddy) | 18,614 | $0.111 | $0.027 | 76% |
| p1 (Proofreading) | 65,153 | $0.350 | $0.057 | 84% |
| r1 (Column) | 7,232 | $0.064 | $0.032 | 50% |
| w1 (Bodo) | 19,476 | $0.117 | $0.030 | 74% |
| t1 (Title) | 74,615 | $0.388 | $0.052 | 87% |
| f1 (Foreign) | 63,409 | $0.349 | $0.064 | 82% |

### 4.3 Monthly/Annual Projection (1,000 requests/service)

| Service | Before | After | Monthly Savings |
|---------|--------|-------|-----------------|
| b1 | $111 | $27 | $84 |
| p1 | $350 | $57 | $293 |
| r1 | $64 | $32 | $32 |
| w1 | $117 | $30 | $87 |
| t1 | $388 | $52 | $336 |
| f1 | $349 | $64 | $285 |
| **Total** | **$1,379** | **$262** | **$1,117** |

| Metric | Value |
|--------|-------|
| Monthly Savings | $1,117 |
| **Annual Savings** | **$13,404** |

---

## 5. Test Results

### 5.1 Test Results (December 21, 2025)

| Service | Time (KST) | Result | Cache Tokens |
|---------|------------|--------|--------------|
| b1 | 20:00 | 4/4 HIT | 18,614 |
| p1 | 19:56 | 4/4 HIT | 65,153 |
| r1 | 19:49 | 4/4 HIT | 7,232 |
| w1 | 20:14 | 4/4 HIT | 19,476 |
| t1 | 19:00 | 4/4 HIT | 74,615 |
| f1 | 19:49 | 4/4 HIT | 63,409 |

### 5.2 Cache Behavior Analysis

| Pattern | Cause | Impact | Mitigation |
|---------|-------|--------|------------|
| Cold Start MISS | First request after deployment | One-time cost | Expected behavior |
| Engine Type Change | Different prompt variants | Creates new cache | Design consideration |
| 100% HIT after warmup | Cache properly configured | Maximum savings | Target state |

---

## 6. Key Files Modified

| Service | anthropic_client.py | websocket_service.py |
|---------|--------------------|--------------------|
| b1 | `services/buddy/external/two/backend/lib/` | `services/buddy/external/two/backend/services/` |
| p1 | `services/proofreading/external/two/backend/lib/` | `services/proofreading/external/two/backend/services/` |
| r1 | `services/regression/external/two/backend/lib/` | `services/regression/external/two/backend/services/` |
| w1 | `services/bodo/external/two/backend/lib/` | `services/bodo/external/two/backend/services/` |
| t1 | `services/title/external/two/backend/lib/` | `services/title/external/two/backend/services/` |
| f1 | `services/foreign/external/two/backend/lib/` | `services/foreign/external/two/backend/services/` |

---

## 7. Deployment Commands

| Service | Command |
|---------|---------|
| b1 | `cd /nexus/services/buddy/external/two && ./deploy-backend.sh` |
| p1 | `cd /nexus/services/proofreading/external/two && ./deploy-anthropic.sh` |
| r1 | `cd /nexus/services/regression/external/two && ./deploy-backend.sh` |
| w1 | `cd /nexus/services/bodo/external/two/w1-scripts && ./deploy-backend.sh` |
| t1 | `cd /nexus/services/title/external/two && ./deploy-backend.sh` |
| f1 | `cd /nexus/services/foreign/external/two/f1-scripts && ./deploy-*.sh` |

---

## 8. Monitoring

### 8.1 Log Commands

```bash
# Example for p1 service
aws logs tail /aws/lambda/nx-wt-prf-websocket-message \
    --region us-east-1 --follow | grep -E "CACHE|HIT|MISS|Token|Cost"
```

### 8.2 Log Patterns

| Pattern | Meaning |
|---------|---------|
| `âœ… Cache HIT for {engine}` | In-memory cache hit (Layer 1) |
| `ðŸŽ¯ PROMPT CACHE HIT!` | Anthropic cache hit (Layer 2) |
| `ðŸ’¾ PROMPT CACHE MISS!` | Anthropic cache miss |
| `ðŸ’° Cost Breakdown:` | Per-request cost details |

---

## 9. Conclusion

### 9.1 Summary of Changes

| Component | Before | After |
|-----------|--------|-------|
| In-Memory Cache TTL | 5 minutes | Permanent |
| DynamoDB Queries | Every 5 min | Cold start only |
| Anthropic Caching | Not used | Enabled (ephemeral) |
| Token Cost (cached) | $5.00/1M | $0.50/1M |
| Monthly Cost (6 services) | $1,389 | $262 |

### 9.2 Results Achieved

- **81% monthly cost reduction** ($1,127 saved)
- **$13,404 annual savings** projection
- **100% cache hit rate** after warmup
- **Dual-layer caching** for maximum efficiency

### 9.3 Recommendations

1. **Monitor cache efficiency** weekly to identify any degradation
2. **Document prompt changes** that may affect cache keys
3. **Consider implementing cache warmup** scripts for production deployments
4. **Plan for engine type variants** if multiple prompt types are needed per service

---

## Appendix A: Changelog

### December 22, 2025
- f1 Lambda timeout: 120s â†’ 180s
- Documentation updates

### December 21, 2025
- Initial caching implementation for all 6 services
- w1: Fixed conversation_context cache invalidation
- b1: Added anthropic-beta header
- p1, r1, t1, f1: Fixed logger and cache_control parameters

---

## Appendix B: Reference Documentation

- [Anthropic Prompt Caching Documentation](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching)
- [Claude API Reference](https://docs.anthropic.com/en/api/messages)
- Internal: `/nexus/docs/cost-optimization/README.md`

---

*Report generated by Development Team*
*SEDAILY.AI - Seoul Economic Daily*
