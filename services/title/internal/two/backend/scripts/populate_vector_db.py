"""
S3 í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì½ì–´ì„œ Aurora PostgreSQLì— ë²¡í„° ì €ì¥
Titan Embeddingsë¥¼ ì‚¬ìš©í•œ ë²¡í„°í™” ë° pgvector ì¸ë±ì‹±
"""
import boto3
import psycopg2
from psycopg2.extras import execute_values
import json
from typing import List, Dict
import time
import os

# AWS ì„¤ì •
S3_BUCKET = 'nx-tt-knowledge-base'
S3_PREFIX = 'prompts/'

# Aurora ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê¸°)
DB_CONFIG = {
    'host': os.environ.get('AURORA_HOST', 'nx-tt-vector-db.cluster-c83iuyksky7r.us-east-1.rds.amazonaws.com'),
    'database': os.environ.get('AURORA_DB', 'vectordb'),
    'user': os.environ.get('AURORA_USER', 'postgres'),
    'password': os.environ.get('AURORA_PASSWORD', 'NexusRAG2024VectorDB'),
    'port': int(os.environ.get('AURORA_PORT', '5432'))
}

# Bedrock í´ë¼ì´ì–¸íŠ¸
bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')
s3 = boto3.client('s3', region_name='us-east-1')


def get_db_connection():
    """Aurora PostgreSQL ì—°ê²°"""
    return psycopg2.connect(**DB_CONFIG)


def chunk_text(text: str, max_tokens: int = 500) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• 

    ì „ëµ:
    1. XML íƒœê·¸ ê¸°ë°˜ ë¶„í•  (í”„ë¡¬í”„íŠ¸ê°€ XML í˜•ì‹ì´ë¯€ë¡œ)
    2. ë¬¸ë‹¨ ê¸°ë°˜ ë¶„í• 
    3. max_tokens ì œí•œ ì¤€ìˆ˜

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        max_tokens: ìµœëŒ€ í† í° ìˆ˜ (1 í† í° â‰ˆ 4 ë¬¸ì)

    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    # XML ì„¹ì…˜ ë¶„í•  ì‹œë„
    import re

    # XML íƒœê·¸ë¡œ ì„¹ì…˜ êµ¬ë¶„
    # <principle>, <technique>, <method> ë“±
    xml_pattern = r'<(\w+)[^>]*>.*?</\1>'
    sections = re.findall(xml_pattern, text, re.DOTALL)

    if sections:
        # XML ì„¹ì…˜ ê¸°ë°˜ ë¶„í• 
        chunks = []
        for match in re.finditer(xml_pattern, text, re.DOTALL):
            section_text = match.group(0)
            estimated_tokens = len(section_text) / 4

            if estimated_tokens > max_tokens:
                # ë„ˆë¬´ í¬ë©´ ë¬¸ë‹¨ìœ¼ë¡œ ì¬ë¶„í• 
                sub_chunks = _chunk_by_paragraphs(section_text, max_tokens)
                chunks.extend(sub_chunks)
            else:
                chunks.append(section_text)

        return chunks if chunks else [text]
    else:
        # XMLì´ ì•„ë‹ˆë©´ ë¬¸ë‹¨ ê¸°ë°˜ ë¶„í• 
        return _chunk_by_paragraphs(text, max_tokens)


def _chunk_by_paragraphs(text: str, max_tokens: int) -> List[str]:
    """ë¬¸ë‹¨ ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• """
    paragraphs = text.split('\n\n')

    chunks = []
    current_chunk = ""

    for para in paragraphs:
        para = para.strip()
        if not para:
            continue

        estimated_tokens = len(current_chunk + para) / 4

        if estimated_tokens > max_tokens and current_chunk:
            chunks.append(current_chunk.strip())
            current_chunk = para
        else:
            current_chunk += "\n\n" + para if current_chunk else para

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks if chunks else [text]


def get_embedding(text: str) -> List[float]:
    """
    Bedrock Titan Embeddingsë¡œ ë²¡í„° ìƒì„±

    Args:
        text: ë²¡í„°í™”í•  í…ìŠ¤íŠ¸ (ìµœëŒ€ 25,000 ë¬¸ì)

    Returns:
        1536 ì°¨ì› ë²¡í„°
    """
    # Titan ì œí•œ: 25K ë¬¸ì
    truncated_text = text[:25000]

    response = bedrock_runtime.invoke_model(
        modelId='amazon.titan-embed-text-v1',
        body=json.dumps({
            "inputText": truncated_text
        })
    )

    result = json.loads(response['body'].read())
    return result['embedding']


def process_file(file_key: str, conn) -> int:
    """
    S3 íŒŒì¼ í•˜ë‚˜ë¥¼ ì²˜ë¦¬í•˜ì—¬ Auroraì— ì €ì¥

    Args:
        file_key: S3 íŒŒì¼ í‚¤
        conn: PostgreSQL ì—°ê²°

    Returns:
        ì €ì¥ëœ ì²­í¬ ìˆ˜
    """
    print(f"\nğŸ“„ Processing: {file_key}")

    # S3ì—ì„œ íŒŒì¼ ì½ê¸°
    response = s3.get_object(Bucket=S3_BUCKET, Key=file_key)
    content = response['Body'].read().decode('utf-8')

    # íŒŒì¼ëª… ì¶”ì¶œ
    source_file = file_key.split('/')[-1]

    # í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• 
    chunks = chunk_text(content, max_tokens=500)
    print(f"   â†’ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")

    # ê° ì²­í¬ ì²˜ë¦¬
    cursor = conn.cursor()
    inserted_count = 0

    for idx, chunk in enumerate(chunks):
        try:
            # ì„ë² ë”© ìƒì„±
            embedding = get_embedding(chunk)

            # ë©”íƒ€ë°ì´í„°
            metadata = {
                'source_type': 'core' if 'core' in file_key else 'knowledge',
                'chunk_size': len(chunk),
                'chunk_tokens': int(len(chunk) / 4),
                'file_key': file_key
            }

            # DBì— ì €ì¥
            cursor.execute(
                """
                INSERT INTO prompt_chunks
                (source_file, chunk_index, content, embedding, metadata)
                VALUES (%s, %s, %s, %s, %s)
                """,
                (source_file, idx, chunk, embedding, json.dumps(metadata))
            )

            inserted_count += 1

            if (idx + 1) % 10 == 0:
                print(f"   â†’ {idx + 1}/{len(chunks)} ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ")
                conn.commit()

                # Rate limiting (Titan Embeddings: 1000 req/min)
                time.sleep(0.1)

        except Exception as e:
            print(f"   âŒ ì²­í¬ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            continue

    conn.commit()
    cursor.close()

    print(f"   âœ… {source_file}: {inserted_count}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")
    return inserted_count


def create_tables(conn):
    """ë²¡í„° í…Œì´ë¸” ë° ì¸ë±ìŠ¤ ìƒì„±"""
    cursor = conn.cursor()

    print("\n[1/3] pgvector Extension í™œì„±í™”")
    cursor.execute("CREATE EXTENSION IF NOT EXISTS vector")
    conn.commit()
    print("   âœ… pgvector extension í™œì„±í™” ì™„ë£Œ")

    print("\n[2/3] í…Œì´ë¸” ìƒì„±")
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS prompt_chunks (
            id SERIAL PRIMARY KEY,
            source_file VARCHAR(255) NOT NULL,
            chunk_index INTEGER NOT NULL,
            content TEXT NOT NULL,
            embedding vector(1536),
            metadata JSONB,
            created_at TIMESTAMP DEFAULT NOW(),
            updated_at TIMESTAMP DEFAULT NOW()
        )
    """)
    conn.commit()
    print("   âœ… prompt_chunks í…Œì´ë¸” ìƒì„± ì™„ë£Œ")

    print("\n[3/3] ì¸ë±ìŠ¤ ìƒì„±")

    # ë²¡í„° ì¸ë±ìŠ¤ (ivfflat)
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_embedding
        ON prompt_chunks
        USING ivfflat (embedding vector_cosine_ops)
        WITH (lists = 100)
    """)

    # ì¼ë°˜ ì¸ë±ìŠ¤
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_source_file
        ON prompt_chunks(source_file)
    """)

    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_metadata
        ON prompt_chunks USING GIN(metadata)
    """)

    conn.commit()
    cursor.close()
    print("   âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("="*60)
    print("ğŸš€ S3 â†’ Aurora PostgreSQL ë²¡í„° ì €ì¥")
    print("="*60)

    # DB ì—°ê²°
    try:
        conn = get_db_connection()
        print("âœ… Aurora PostgreSQL ì—°ê²° ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        return

    # í…Œì´ë¸” ìƒì„±
    try:
        create_tables(conn)
    except Exception as e:
        print(f"âŒ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
        conn.close()
        return

    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì—¬ë¶€ í™•ì¸
    cursor = conn.cursor()
    cursor.execute("SELECT COUNT(*) FROM prompt_chunks")
    existing_count = cursor.fetchone()[0]
    cursor.close()

    if existing_count > 0:
        print(f"\nâš ï¸  ê¸°ì¡´ ë°ì´í„° {existing_count}ê°œê°€ ìˆìŠµë‹ˆë‹¤. ìë™ìœ¼ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.")
        # ìë™ìœ¼ë¡œ ê±´ë„ˆëœ€ (ê¸°ì¡´ ë°ì´í„° ìœ ì§€)

    # S3 íŒŒì¼ ëª©ë¡ ì¡°íšŒ
    print("\n" + "="*60)
    print("ğŸ“ S3 íŒŒì¼ ì¡°íšŒ")
    print("="*60)

    response = s3.list_objects_v2(
        Bucket=S3_BUCKET,
        Prefix=S3_PREFIX
    )

    files = [obj['Key'] for obj in response.get('Contents', [])
             if obj['Key'].endswith('.txt')]

    print(f"âœ… ì²˜ë¦¬í•  íŒŒì¼: {len(files)}ê°œ")

    # ê° íŒŒì¼ ì²˜ë¦¬
    print("\n" + "="*60)
    print("ğŸ”„ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘")
    print("="*60)

    total_chunks = 0
    start_time = time.time()

    for i, file_key in enumerate(files, 1):
        print(f"\n[{i}/{len(files)}]", end=" ")
        try:
            count = process_file(file_key, conn)
            total_chunks += count
        except Exception as e:
            print(f"   âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            continue

    elapsed = time.time() - start_time

    # í†µê³„ ì¡°íšŒ
    cursor = conn.cursor()
    cursor.execute("SELECT COUNT(*) FROM prompt_chunks")
    total_count = cursor.fetchone()[0]

    cursor.execute("""
        SELECT source_file, COUNT(*)
        FROM prompt_chunks
        GROUP BY source_file
        ORDER BY COUNT(*) DESC
    """)
    stats = cursor.fetchall()

    cursor.close()
    conn.close()

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("âœ… ë²¡í„° ì €ì¥ ì™„ë£Œ!")
    print("="*60)
    print(f"\nì´ ì²­í¬ ìˆ˜: {total_count}ê°œ")
    print(f"ì²˜ë¦¬ ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print(f"í‰ê·  ì²˜ë¦¬ ì†ë„: {total_count / elapsed:.1f} ì²­í¬/ì´ˆ")
    print("\níŒŒì¼ë³„ í†µê³„:")
    for source_file, count in stats:
        print(f"  - {source_file}: {count}ê°œ")

    print("\n" + "="*60)
    print("ë‹¤ìŒ ë‹¨ê³„:")
    print("1. Lambda í™˜ê²½ë³€ìˆ˜ ì„¤ì •:")
    print(f"   AURORA_HOST={DB_CONFIG['host']}")
    print(f"   AURORA_DB={DB_CONFIG['database']}")
    print(f"   AURORA_USER={DB_CONFIG['user']}")
    print("   AURORA_PASSWORD=<Secrets Manager>")
    print("   USE_RAG=true  # RAG í™œì„±í™”")
    print("2. Lambda VPC ì„¤ì • (Auroraì™€ ë™ì¼ VPC)")
    print("3. Lambda Security Group â†’ Aurora SG ì ‘ê·¼ í—ˆìš©")
    print("4. requirements.txtì— psycopg2-binary ì¶”ê°€")
    print("="*60)


if __name__ == "__main__":
    main()
