"""
Aurora PostgreSQL pgvectorë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê²€ìƒ‰
ë²¡í„° ê¸°ë°˜ RAG (Retrieval-Augmented Generation) êµ¬í˜„
"""
import psycopg2
from psycopg2.pool import SimpleConnectionPool
import json
import boto3
from typing import List, Dict, Optional
import os
import sys

# utils ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.logger import setup_logger

logger = setup_logger(__name__)


class VectorSearch:
    """
    Aurora PostgreSQL pgvector ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ í´ë˜ìŠ¤
    """

    def __init__(self):
        """ì´ˆê¸°í™” ë° DB ì—°ê²° í’€ ìƒì„±"""
        # DB ì„¤ì •
        self.db_config = {
            'host': os.environ.get('AURORA_HOST'),
            'database': os.environ.get('AURORA_DB', 'vectordb'),
            'user': os.environ.get('AURORA_USER', 'postgres'),
            'password': os.environ.get('AURORA_PASSWORD'),
            'port': int(os.environ.get('AURORA_PORT', '5432'))
        }

        # ì—°ê²° í’€ ìƒì„± (Lambda ì»¨í…Œì´ë„ˆ ì¬ì‚¬ìš© ì‹œ íš¨ìœ¨ì )
        try:
            self.pool = SimpleConnectionPool(
                minconn=1,
                maxconn=5,
                **self.db_config
            )
            logger.info("âœ… Aurora PostgreSQL ì—°ê²° í’€ ìƒì„± ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ DB ì—°ê²° í’€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            self.pool = None

        # Bedrock Runtime í´ë¼ì´ì–¸íŠ¸
        self.bedrock_runtime = boto3.client(
            'bedrock-runtime',
            region_name=os.environ.get('AWS_REGION', 'us-east-1')
        )

    def get_embedding(self, text: str) -> List[float]:
        """
        Bedrock Titan Embeddingsë¡œ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            text: ë²¡í„°í™”í•  í…ìŠ¤íŠ¸

        Returns:
            1536 ì°¨ì› ë²¡í„°
        """
        try:
            response = self.bedrock_runtime.invoke_model(
                modelId='amazon.titan-embed-text-v1',
                body=json.dumps({
                    "inputText": text[:25000]  # Titan ì œí•œ: 25K ë¬¸ì
                })
            )

            result = json.loads(response['body'].read())
            embedding = result['embedding']

            logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embedding)} ì°¨ì›")
            return embedding

        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise

    def search_similar_chunks(
        self,
        query: str,
        top_k: int = 10,
        min_similarity: float = 0.7,
        source_filter: Optional[List[str]] = None
    ) -> List[Dict]:
        """
        ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ í”„ë¡¬í”„íŠ¸ ì²­í¬ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ (ê¸°ì‚¬ ë‚´ìš©)
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            min_similarity: ìµœì†Œ ìœ ì‚¬ë„ (0-1)
            source_filter: íŠ¹ì • ì†ŒìŠ¤ íŒŒì¼ë§Œ ê²€ìƒ‰ (ì˜ˆ: ['instruction.txt'])

        Returns:
            ìœ ì‚¬í•œ ì²­í¬ ë¦¬ìŠ¤íŠ¸ [{id, source_file, content, similarity, ...}]
        """
        if not self.pool:
            logger.error("âŒ DB ì—°ê²° í’€ì´ ì—†ìŠµë‹ˆë‹¤")
            return []

        try:
            # 1. ì¿¼ë¦¬ ë²¡í„°í™”
            logger.info(f"ğŸ” ë²¡í„° ê²€ìƒ‰ ì‹œì‘: query_length={len(query)}, top_k={top_k}")
            query_embedding = self.get_embedding(query)

            # 2. DB ì—°ê²° ê°€ì ¸ì˜¤ê¸°
            conn = self.pool.getconn()
            cursor = conn.cursor()

            # 3. SQL ì¿¼ë¦¬ êµ¬ì„±
            # pgvector: <=> ì—°ì‚°ìëŠ” ì½”ì‚¬ì¸ ê±°ë¦¬ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬)
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = 1 - ì½”ì‚¬ì¸ ê±°ë¦¬
            sql = """
                SELECT
                    id,
                    source_file,
                    chunk_index,
                    content,
                    metadata,
                    1 - (embedding <=> %s::vector) AS similarity
                FROM prompt_chunks
                WHERE 1 - (embedding <=> %s::vector) >= %s
            """

            params = [query_embedding, query_embedding, min_similarity]

            # ì†ŒìŠ¤ í•„í„° ì¶”ê°€
            if source_filter:
                placeholders = ','.join(['%s'] * len(source_filter))
                sql += f" AND source_file IN ({placeholders})"
                params.extend(source_filter)

            sql += """
                ORDER BY embedding <=> %s::vector
                LIMIT %s
            """
            params.extend([query_embedding, top_k])

            # 4. ì¿¼ë¦¬ ì‹¤í–‰
            cursor.execute(sql, params)
            rows = cursor.fetchall()

            # 5. ê²°ê³¼ êµ¬ì„±
            results = []
            for row in rows:
                results.append({
                    'id': row[0],
                    'source_file': row[1],
                    'chunk_index': row[2],
                    'content': row[3],
                    'metadata': row[4],
                    'similarity': float(row[5])
                })

            # 6. ì—°ê²° ë°˜í™˜
            cursor.close()
            self.pool.putconn(conn)

            logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ì²­í¬ ë°œê²¬")
            for i, result in enumerate(results[:3], 1):
                logger.info(f"   [{i}] {result['source_file']} (ìœ ì‚¬ë„: {result['similarity']:.2%})")

            return results

        except Exception as e:
            logger.error(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            if conn:
                self.pool.putconn(conn)
            return []

    def build_context_from_results(
        self,
        results: List[Dict],
        max_tokens: int = 10000
    ) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜

        í† í° ì œí•œì— ë§ì¶° ê°€ì¥ ê´€ë ¨ë„ ë†’ì€ ì²­í¬ë“¤ë§Œ í¬í•¨

        Args:
            results: search_similar_chunks() ê²°ê³¼
            max_tokens: ìµœëŒ€ í† í° ìˆ˜

        Returns:
            í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•  ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        if not results:
            logger.warning("âš ï¸  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            return ""

        context_parts = []
        total_tokens = 0

        for result in results:
            content = result['content']
            # ëŒ€ëµì ì¸ í† í° ìˆ˜ ê³„ì‚° (1 í† í° â‰ˆ 4 ë¬¸ì)
            estimated_tokens = len(content) / 4

            if total_tokens + estimated_tokens > max_tokens:
                logger.info(f"âš ï¸  í† í° ì œí•œ ë„ë‹¬: {total_tokens}/{max_tokens}")
                break

            context_parts.append({
                'source': result['source_file'],
                'chunk_index': result['chunk_index'],
                'similarity': result['similarity'],
                'content': content
            })

            total_tokens += estimated_tokens

        # ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ êµ¬ì„±
        context = "## ğŸ“š ê´€ë ¨ í”„ë¡¬í”„íŠ¸ ì§€ì¹¨ (RAG ê²€ìƒ‰ ê²°ê³¼)\n\n"
        context += f"ê²€ìƒ‰ëœ ì²­í¬: {len(context_parts)}ê°œ (ì´ ~{int(total_tokens)} í† í°)\n\n"

        for i, part in enumerate(context_parts, 1):
            context += f"### [{i}] {part['source']} - ì²­í¬ #{part['chunk_index']} (ìœ ì‚¬ë„: {part['similarity']:.2%})\n\n"
            context += f"{part['content']}\n\n"
            context += "---\n\n"

        logger.info(f"âœ… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {len(context_parts)}ê°œ ì²­í¬, ~{int(total_tokens)} í† í°")

        return context

    def get_stats(self) -> Dict:
        """
        ë²¡í„° DB í†µê³„ ì¡°íšŒ

        Returns:
            í†µê³„ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        if not self.pool:
            return {'error': 'DB ì—°ê²° ì—†ìŒ'}

        try:
            conn = self.pool.getconn()
            cursor = conn.cursor()

            # ì „ì²´ ì²­í¬ ìˆ˜
            cursor.execute("SELECT COUNT(*) FROM prompt_chunks")
            total_chunks = cursor.fetchone()[0]

            # íŒŒì¼ë³„ ì²­í¬ ìˆ˜
            cursor.execute("""
                SELECT source_file, COUNT(*)
                FROM prompt_chunks
                GROUP BY source_file
                ORDER BY COUNT(*) DESC
            """)
            by_source = {row[0]: row[1] for row in cursor.fetchall()}

            # í‰ê·  ì²­í¬ í¬ê¸°
            cursor.execute("""
                SELECT AVG(LENGTH(content))
                FROM prompt_chunks
            """)
            avg_chunk_size = cursor.fetchone()[0]

            cursor.close()
            self.pool.putconn(conn)

            return {
                'total_chunks': total_chunks,
                'by_source': by_source,
                'avg_chunk_size': int(avg_chunk_size) if avg_chunk_size else 0
            }

        except Exception as e:
            logger.error(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {'error': str(e)}

    def close(self):
        """ì—°ê²° í’€ ì¢…ë£Œ"""
        if self.pool:
            self.pool.closeall()
            logger.info("âœ… DB ì—°ê²° í’€ ì¢…ë£Œ")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (Lambda ì»¨í…Œì´ë„ˆ ì¬ì‚¬ìš©)
_vector_search_instance = None


def get_vector_search() -> VectorSearch:
    """
    VectorSearch ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    Lambda ì»¨í…Œì´ë„ˆ ì¬ì‚¬ìš© ì‹œ ì—°ê²° í’€ ìœ ì§€
    """
    global _vector_search_instance

    if _vector_search_instance is None:
        _vector_search_instance = VectorSearch()

    return _vector_search_instance
